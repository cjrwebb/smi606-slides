---
title: "Week 10 Exercise: Cluster Analysis"
author: "Calum Webb"
date: "27/11/2021"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

In this practical activity we will be using cluster analysis to try and explore whether there are underlying clusters of crime incidence in US states and English Community Safety Partnerships, using data from the CORGIS Dataset Project (https://corgis-edu.github.io/corgis/csv/state_crime/) and the Office for National Statistics (https://www.ons.gov.uk/peoplepopulationandcommunity/crimeandjustice/datasets/recordedcrimedatabycommunitysafetypartnershiparea). The researchers in this activity are interested in finding out if states/community partnerships simply cluster into 'high' or 'low' crime, or whether there are clusters of specific types of crime (assault, murder, sexual crime, theft, etc.)

First, you will be asked to follow along and interpret the output from some code analysing clusters of crime in US states. Then, you will be asked to use this code as a template to explore clusters of crime in community safety partnerships in England.

## Part I: Clusters of Crime Types in US States

Start by loading (or installing and then loading) the relevant libraries used for cluster analysis. 

```{r}

# Don't forget to install any packages you don't have installed.
#install.packages("tidyverse")
#install.packages("cluster")
#install.packages("factoextra")

library(tidyverse)
library(cluster)
library(factoextra)

```

First, the researchers read in the data and save it in an object called `usa_data`

```{r}

usa_data <- read_csv("state_crime_rates.csv")
usa_data 

```

Next, the researchers decide that because their variables of interest are all continuous they will start by using k-means to try and identify relevant clusters. They start by removing any non-numeric variables from their dataset, keeping only the numeric ones.

```{r}

usa_data_prepped <- usa_data %>%
  select(-state)

```

It's generally also recommended to standardise all of the continuous variables that you are using in a cluster analysis too, as what might look like big gaps between data points might just be linked to the scale that they're on (e.g. the difference between measuring something as being 1000 milimeters away compared to 1 meter). This can be achieved using the `scale()` function. Don't worry, we'll be adding our clusters back onto our original data so we'll still be able to make sense of the differences between groups.

```{r}

usa_data_prepped <- usa_data_prepped %>%
  mutate_all(scale)

```

They start by using the `factoextra` package, and the `fviz_nbclust` function to try and determine how many clusters they should try to identify.

```{r}

# Silhouette Plot
fviz_nbclust(x = usa_data_prepped, 
             FUNcluster = kmeans, 
             method = "silhouette")

# Gap statistic plot
fviz_nbclust(x = usa_data_prepped, 
             FUNcluster = kmeans, 
             method = "gap")

```

* Interpret the above plots: what is the optimal number of clusters according to the silhouette statistic and what is the optimal number of clusters according to the gap statistic?


The optimal number of clusters according to the silhouette statistic is 2, whereas the optimal number of clusters according to the gap statistic is 3.



---

The researchers decide that they will create a 2-cluster solution (as suggested by the silhouette plot), as well as a 3-cluster solution suggested by the gap statistic. They use the `kmeans` to first estimate the clusters, they then visualise the clusters using the `fviz_cluster` function.

```{r}

# run kmeans analysis
set.seed(2021)
usa_k2 <- kmeans(usa_data_prepped, centers = 2)
usa_k2

# run kmeans analysis
set.seed(2021)
usa_k3 <- kmeans(usa_data_prepped, centers = 3)
usa_k3

# visualise clusters - 2 cluster
fviz_cluster(usa_k2,
             data = usa_data_prepped)

# visualise clusters - 3 cluster
fviz_cluster(usa_k3,
             data = usa_data_prepped)

```

* Approximately what proportion of variance could be explained by the cluster membership solution chosen in the 2-cluster and 3-cluster solutions?

The proportion of variance explained in the 2 cluster solution was around 45.9%, whereas the proportion explained in the 3 cluster solution was 60.4%.



* Do the clusters look well defined? Are there any states that may have been misclassified by the algorithm?

Both 2-clusters and 3 cluster solutions appear to have minimal overlap. In the three cluster solution, there is some ambiguity as to whether state 24 (Mississippi) and state 5 (California) belong in Cluster 1 or Cluster 2.



---

The researchers then decide to add the cluster membership to the original data and then explore how the clusters differ in mean values of each crime rate.

```{r}

# Add cluster results to data and save in `usa_data_results`
usa_data_results <- usa_data %>%
  mutate(
    cluster_k2 = usa_k2$cluster,
    cluster_k3 = usa_k3$cluster
  )

# Summarise all numeric variables with their mean
# Tip: Uncomment the %>% view() section of the code to view all output
usa_data_results %>%
  group_by(cluster_k2) %>%
  summarise_all(~mean(., na.rm = TRUE)) # %>% view()

usa_data_results %>%
  group_by(cluster_k3) %>%
  summarise_all(~mean(., na.rm = TRUE)) # %>% view()

```

* Describe and label the two kinds of clusters found in the 2-cluster solution.

The two cluster solution appears to be broken into high-crime and low-crime clusters. Cluster 1 has higher average incidence of all forms of recorded crime whereas cluster 2 has lower average incidence of all forms of crime.


* Describe and label the three kinds of clusters found in the 3-cluster solution.

The three cluster solution appears to have clustered the states into low, middling, and high incidence of crime, with cluster 3 being the lowest and cluster 2 being the highest. The exception is that cluster 1 seems to have the highest incidences of violent robberies, which may be more of a unique feature of this cluster.


---

Some plots have been created to help visualise the differences between clusters more easily. This is achieved using the `pivot_longer` function to first put all of the variables on their own rows. Notice how this is done using the standardised scores so that they are easier to compare. Look at how the structure of the dataset changes:

```{r}

# Create a data frame from our results. Save our cluster in
# the variable "cluster"
k2_results <- as_tibble(usa_k2$centers) %>%
  rowid_to_column(var = "cluster")
k2_results

k2_results_long <- k2_results %>%
  # make all of our variable, but not our cluster ID, long format. 
  # Make our variable column name "variable" and our value name "score"
  pivot_longer(-cluster, names_to = "variable", values_to = "score") 

k2_results_long

k2_results_long %>%
  ggplot() +
  geom_col(aes(x = score, y = variable, fill = factor(cluster)),
           position = "dodge") +
  scale_fill_viridis_d() # make the scale more colourblind friendly

```

Of course, with only two clusters this isn't much more helpful than just looking at the table. But the visualisations can be more useful if we want to use multiple clusters. 

Try copy and pasting the code above to the R chunk below and editing it so that it visualises the three cluster solution instead:

```{r}

# Create a data frame from our results. Save our cluster in
# the variable "cluster"
k3_results <- as_tibble(usa_k3$centers) %>%
  rowid_to_column(var = "cluster")
k3_results

k3_results_long <- k3_results %>%
  # make all of our variable, but not our cluster ID, long format. 
  # Make our variable column name "variable" and our value name "score"
  pivot_longer(-cluster, names_to = "variable", values_to = "score") 

k3_results_long

k3_results_long %>%
  ggplot() +
  geom_col(aes(x = score, y = variable, fill = factor(cluster)),
           position = "dodge") +
  scale_fill_viridis_d() # make the scale more colourblind friendly

```


---

The researchers then decide to check whether they find similar results when using hierarchical cluster analysis. 

They decide that since all of their data is continuous, they will create a dissimilarity matrix based on Euclidean distance.

```{r}

usa_d <- daisy(as.matrix(usa_data_prepped), metric = "euclidean")

```

They decide to use two different methods for hierarchical cluster analysis: the Ward's linkage method and complete linkage method.

```{r}

set.seed(2021)
usa_ward     <- hclust(d = usa_d, method = "ward.D")
set.seed(2021)
usa_complete <- hclust(d = usa_d, method = "complete")

```

Then then visualise a dendrogram of their results.

```{r}

plot(usa_ward)

plot(usa_complete)

```

* Based on the two dendrograms, how many different clusters might be reasonable to extract from the data and why?

HCA using Ward linkage seems to show two very large clusters, but there could be good arguments for a three cluster solution. Similarly, with complete linkage there is a good argument for a two cluster solution, a three cluster solution with two "outlier" states, or a more complex 6 cluster solution. 



---

The researchers decide to test a 3-cluster (Ward and complete) solution to their data clustering. They use the `cutree` function to achieve this.

```{r}

usa_ward_k3 <- cutree(usa_ward, k = 3)
usa_comp_k3 <- cutree(usa_complete, k = 3)

```

They add the cluster results to their data and generate some descriptive statistics for each solution.

```{r}

usa_data_hca_results <- usa_data %>%
  mutate(
    ward_k3 = usa_ward_k3,
    comp_k3 = usa_comp_k3
  )

# Results for 3 cluster ward
usa_data_hca_results %>%
  group_by(ward_k3) %>%
  summarise_all(~mean(., na.rm = TRUE)) # %>% view()

# Results for 3 cluster complete
usa_data_hca_results %>%
  group_by(comp_k3) %>%
  summarise_all(~mean(., na.rm = TRUE)) # %>% view()


```


* Write a description and labels for the 3-cluster solution found through Ward's linkage

The Ward linkage HCA appears to show three clusters of states that could be labelled high crime (1), medium crime (2), and low crime (3), with the exception of violent robberies.

* Write a description and labels for the 3-cluster solution found through complete linkage

Complete data linkage appears to have found clusters very similar to Ward linkage and k-means, but the very high cluster has been limited to two states and therefore stands out more. Cluster two has the highest rate of all crimes, and followed by cluster 1 and then by cluster 3, which has the lowest rates. 


* How would you summarise the research? Did the researchers find evidence that state-level crime fell into distinct categories of crimes committed, or did clusters largely reflect rates of all crimes?


Cluster analysis did not seem to suggest that, at the state level, communities face very different clusters characterised by types of crime. We might have expected that some states face more difficulties with property crimes while others face more problems with violent crimes, but the evidence from the cluster analysis did not seem to support this. Rather, the clustering of crime was associated with the prevalence of all forms of crime rather than specific forms. 

In the Ward Linkage, Cluster 1 contained states with the highest levels of crime. These tended to be states with high levels of poverty, including Alabama, Mississippi, and Arkansas. States in the cluster with the lowest crime rates tended to be concentrated in the North East and included, for example, New York, New Jersey, New Hampshire, and Vermont, but there were also some unexpected states such as West Virginia.

---

## Part II: Clusters of Crime Types in English Community & Safety Partnerships

Now we'll explore whether we find similar or different results for crime rates in English Community and Safety Partnerships. 

* Start by loading the `"england_crime_rates.csv"` data into `R` using the `read_csv` function. Save the result to an object called `english_crime`.

```{r}

english_crime <- read_csv("england_crime_rates.csv")
english_crime

```

* Check the kinds of variables in the data and decide whether you could use k-means, Hierarchical Cluster Analysis, or both methods for exploring clusters of crime.


Both k-means and Hierarchical Cluster Analysis could be used for analysing this data as all variables of interest are continuous.



---


* Create a version of the data that contains only the numeric type variables and store it in an object called `english_crime_prepped` so that it can be used for k-means and HCA. Then, standardise this dataset using the `scale` function.

```{r}

english_crime_prepped <- english_crime %>%
  select_if(is.numeric)

english_crime_prepped <- scale(english_crime_prepped)

```


---

Let's start with k-means analysis. 

* Use the `fviz_nbclust` function from the `factoextra` package to identify the optimal cluster solution under both the silhouette method and the gap statistic method. 

```{r}

fviz_nbclust(english_crime_prepped, FUNcluster = kmeans, method = "silhouette")

fviz_nbclust(english_crime_prepped, FUNcluster = kmeans, method = "gap")

```

* How many clusters do the silhouette and gap statistic methods recommend respectively?

The silhouette method recommends a three cluster solution and the gap statistic recommends a six cluster solution.


---

* Create a 3-cluster and a 6-cluster solution for the English crime data using the `kmeans` function. Remember to save the results to an object for later use.

```{r}

set.seed(2021)
english_km3 <- kmeans(english_crime_prepped, centers = 3)
set.seed(2021)
english_km6 <- kmeans(english_crime_prepped, centers = 6)

```

* Visualise the 3 cluster and 6 cluster solutions using the `fviz_clusters` function.

```{r}

fviz_cluster(english_km3, data = english_crime_prepped, geom = "point")

fviz_cluster(english_km6, data = english_crime_prepped, geom = "point")

```


* By calling the k-means objects created earlier, report the proportion of variance that can be explained by the 3-cluster solution and the 6-cluster solution.

```{r}

english_km3
english_km6

```


* Do you find any particular solution preferable?

The variance explained in the three cluster solution was 55.7%, compared to 69.7% in the six cluster solution. The added complexity of three additional clusters may not be worth the tradeoff in variance explained.



---

Now, we need to describe the clusters. 

* Get summary statistics for all of the clusters from the 3-cluster and 6- cluster solution that can be used to describe them. You can either: call the cluster centres table directly from the results, add the cluster membership to the original data and then use group_by and summarise, or modify the code for the visualisation we used above and re-purpose it for this visualisation (or even better, have a go at all three!)

```{r}

# get the proportion in each cluster
proportions(table(english_km3$cluster))
proportions(table(english_km6$cluster))

english_crime_km_results <- english_crime %>%
  mutate(
    cluster_km3 = english_km3$cluster,
    cluster_km6 = english_km6$cluster,
  ) 

english_crime_km_results %>%
  group_by(cluster_km3) %>%
  summarise_if(is.numeric, mean) # %>% view()

english_crime_km_results %>%
  group_by(cluster_km6) %>%
  summarise_if(is.numeric, mean) # %>% view()


# Create a data frame from our results. Save our cluster in
# the variable "cluster"
k3_results <- as_tibble(english_km3$centers) %>%
  rowid_to_column(var = "cluster")
k3_results

k3_results_long <- k3_results %>%
  # make all of our variable, but not our cluster ID, long format. 
  # Make our variable column name "variable" and our value name "score"
  pivot_longer(-cluster, names_to = "variable", values_to = "score") 

k3_results_long

k3_results_long %>%
  ggplot() +
  geom_col(aes(x = score, y = variable, fill = factor(cluster)),
           position = "dodge") +
  scale_fill_viridis_d() # make the scale more colourblind friendly

# Create a data frame from our results. Save our cluster in
# the variable "cluster"
k6_results <- as_tibble(english_km6$centers) %>%
  rowid_to_column(var = "cluster")
k6_results

k6_results_long <- k6_results %>%
  # make all of our variable, but not our cluster ID, long format. 
  # Make our variable column name "variable" and our value name "score"
  pivot_longer(-cluster, names_to = "variable", values_to = "score") 

k6_results_long

k6_results_long %>%
  ggplot() +
  geom_col(aes(x = score, y = variable, fill = factor(cluster)),
           position = "dodge") +
  scale_fill_viridis_d() # make the scale more colourblind friendly

```

* Describe the clusters found in the 3-cluster and 6-cluster solution.


__3 Cluster Solution__

The three cluster solution can broadly be described as consisting of a low crime cluster (Cluster 2, green) and two high crime clusters (Cluster 1 and 3, purple and yellow), which differ somewhat in the nature of the crimes committed. For example, Cluster 3 has the highest rate of violence against people, sexual offences, criminal damage and arson, and public order offences than any other cluster, whereas Cluster 1 has far more vehicle, theft, robbery, and drug offences. There is therefore some evidence that high crime areas are split somewhat between violent crimes and property crimes, unlike in the USA. 

Only around 9.6% of areas fall within the first cluster of high property crime areas, while 35.3% of areas fall within the high violent crime areas of cluster 3. Most areas fall within the low crime cluster (55%).

__6 Cluster Solution__

Cluster 1 appears to be characterised by relatively high property, vehicle, and drug crime (high robbery, high theft, high vehicle offences), but with relatively typical levels of other forms of crime and low levels of criminal damage and arson. Cluster 2 is somewhat similar as it has similar elevated levels of property-related offences, but at a much lower level than cluster 1. Both of these clusters have relatively low rates of violent crime and public order offences, and cluster 2 has a particularly low rate of sexual offences. Together, clusters 1 and 2 make up 6.3% and 9.3% of the areas in the data.

Cluster 3 represents areas with low rates of all crimes. These areas make up around 31.3% of all of the areas in the data.

Cluster 4 represents areas with a mixture of violent crime and property crime at a fairly moderately high level. Cluster 4 has average or above average rates of all crimes and represents around 21% of the areas in the data.

Cluster 5 represents areas with atypically low levels of property crime — vehicle offences, theft offences, and robberies — but with fairly average levels of other forms of offences. Around 21.6% of areas were allocated to this cluster. 

Cluster 6 represents high rates of crime across almost all areas, with a mixture of violent and property crime. It represents around 10.3% of the areas in the data. 


* Is there evidence in either of these cluster solutions of areas being clustered into different types of criminal offences, or do clusters only reflect low or high crime as in the United States?


Yes, as opposed to the US data the three cluster solution appears to show the English community safety partnerships can be classified by prevalence of certain types of crime as well as overall crime. 


---

Let's also see if we get similar results from a hierarchical cluster analysis. Before we can do that, we need to pick an appropriate dissimilarity/distance measure and linkage method. Your answers may start to differ from mine here — that's totally fine and to me expected! A lot of cluster analysis is subjective.

* What might be an appropriate distance measure for this data and why? (Hint: see slide 55)

Manhattan distance may be an appropriate measure of dissimilarity due to the large number of variables included in the data (8).


* What might be an appropriate linkage method for this data and why? (Hint: see slide 58)

Ward, Centroid, and Median linkage should be avoided because the distance matrix will be based on Manhattan and now Euclidean distance. Further, single linkage may not be appropriate as we do not assume a 'chain of command' type hierarchy in the data. As such, complete or average linkage may be most appropriate. 


* Create a distance matrix for the English data using the dissimilarity measure of your choice.

```{r}

english_crime_d <- daisy(english_crime_prepped, metric = "manhattan")

```


---

Now we can run the Hierarchical Cluster Analysis algorithm (or algorithms, if trying multiple) that we decided on about.

* Use the `hclust` function to cluster the data according to the linkage method that you chose. Don't forget to save the result to a new object.

```{r}

set.seed(2021)
english_crime_comp <- hclust(english_crime_d, method = "complete")

set.seed(2021)
english_crime_avg <- hclust(english_crime_d, method = "average")

```

* Plot the results of your HCA with a dendrogram using the `plot` function

```{r}

plot(english_crime_comp) # Maybe 4, with one group having only one area (183)

plot(english_crime_avg) # Average linkage doesn't seem to lead to any well-defined groups

```

* Come up with a sensible number of clusters from the above plots that you think the data could be clustered into. Write how many clusters you think there may be in the data based on each dendrogram (if more than one).

Four seems to be an appropriate number of clusters for both dendrograms, though there could be some equally valid larger numbers of clusters. The complete linkage dendrogram appears to have a singleton clade (Community 183), which may indicate outliers or potentially bad fitting clusters. Average linkage doesn't seem to lead to any well-defined groups.

---

Now we can cut our dendrogram into the number of clusters we believe we identified to explore how we might describe them. 

* Use the `cutree` function to cut your dendrogram(s) into the chosen number of clusters.

```{r}

english_hca_comp_results <- cutree(english_crime_comp, k = 4)

```

* Add your cluster solution(s) to the original data using the mutate function and the stored results above.

```{r}

english_crime_hca_results <- english_crime %>%
  mutate(
    hca_comp = english_hca_comp_results,
  )

```

* Now produce some bivariate statistics showing how the crime rates differ by cluster. You can also produce a plot if you think it would be helpful.

```{r}

hca_summary <- english_crime_hca_results %>%
  select(-1, -2) %>%
  group_by(hca_comp) %>%
  summarise_all(~mean(., na.rm = TRUE))# %>% view()

hca_summary

prop.table(table(english_crime_hca_results$hca_comp))

# Create a plot of the HCA results
hca_summary_long <- hca_summary %>%
  # make all of our variables, but not our cluster ID, long format. 
  # Make our variable column name "variable" and our value name "score"
  pivot_longer(-hca_comp, names_to = "variable", values_to = "score") 

hca_summary_long

hca_summary_long %>%
  ggplot() +
  geom_col(aes(x = score, y = variable, fill = factor(hca_comp)),
           position = "dodge") +
  scale_fill_viridis_d() # make the scale more colourblind friendly


```

* Describe the clusters found above (including from multiple linkage methods, if relevant). If possible, label the clusters found.


Complete Linkage Cluster (4)

Cluster 1: The lowest rates of all crimes. (62.3% of all areas)

Cluster 2: The highest rates of violence against the person, criminal damage and arson, and public order offences. (24.3% of all areas)

Cluster 3: Higher rates of property crime than cluster 2, and lower rates of personal/violent crime, crminal damage, and arson. Somewhat elevated rates of drug offences. (13% of all areas)

Cluster 4: This singleton cluster is made up of an area with extremely high levels of theft and drug offences, indicating it is an outlier. This makes sense as inspecting the data shows the area is Westminster.  


* Do these clusters differ from the clusters found using k-means? Which do you prefer as a typology of crime in English community and safety partnerships and why?

The HCA analysis was very similar to the k-means analysis and both have substantively the same interpretations, but it did pick out one particular area which was an outlier, which could be helpful. 



---

## Week 10 Challenge

* Practice using some of the skills we learned in Week 3 (bivariate data visualisation and statistics) to further illustrate the differences and similarities between your favoured cluster analysis of the English crime data. This might make it easier to see the characteristics of clusters than using the means of all variables; it might also show you some interesting differences in terms of variation.

