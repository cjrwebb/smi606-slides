<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>SMI606: Week 10 — Cluster Analysis</title>
    <meta charset="utf-8" />
    <meta name="author" content="Calum Webb" />
    <meta name="date" content="2024-09-16" />
    <script src="week-10-cluster-analysis_files/header-attrs/header-attrs.js"></script>
    <link href="week-10-cluster-analysis_files/tile-view/tile-view.css" rel="stylesheet" />
    <script src="week-10-cluster-analysis_files/tile-view/tile-view.js"></script>
    <link href="week-10-cluster-analysis_files/panelset/panelset.css" rel="stylesheet" />
    <script src="week-10-cluster-analysis_files/panelset/panelset.js"></script>
    <script src="week-10-cluster-analysis_files/xaringanExtra-progressBar/progress-bar.js"></script>
    <link href="week-10-cluster-analysis_files/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <script src="week-10-cluster-analysis_files/clipboard/clipboard.min.js"></script>
    <link href="week-10-cluster-analysis_files/shareon/shareon.min.css" rel="stylesheet" />
    <script src="week-10-cluster-analysis_files/shareon/shareon.min.js"></script>
    <link href="week-10-cluster-analysis_files/xaringanExtra-shareagain/shareagain.css" rel="stylesheet" />
    <script src="week-10-cluster-analysis_files/xaringanExtra-shareagain/shareagain.js"></script>
    <script src="week-10-cluster-analysis_files/mark.js/mark.min.js"></script>
    <link href="week-10-cluster-analysis_files/xaringanExtra-search/search.css" rel="stylesheet" />
    <script src="week-10-cluster-analysis_files/xaringanExtra-search/search.js"></script>
    <script>window.addEventListener('load', function() { window.xeSearch = new RemarkSearch({"position":"bottom-left","caseSensitive":false,"showIcon":true,"autoSearch":false}) })</script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="css/custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: middle
background-size: contain

&lt;br&gt;&lt;br&gt;&lt;br&gt;

# .tuos_purple[SMI606: Week 10&lt;br&gt;Cluster Analysis]

&lt;br&gt;&lt;br&gt;

**Dr. Calum Webb**&lt;br&gt;
Sheffield Methods Institute, the University of Sheffield.&lt;br&gt;
[c.j.webb@sheffield.ac.uk](mailto:c.j.webb@sheffield.ac.uk)





<div>
<style type="text/css">.xaringan-extra-logo {
width: 180px;
height: 128px;
z-index: 0;
background-image: url(header/smi-logo-white.png);
background-size: contain;
background-repeat: no-repeat;
position: absolute;
top:1em;right:2em;
}
</style>
<script>(function () {
  let tries = 0
  function addLogo () {
    if (typeof slideshow === 'undefined') {
      tries += 1
      if (tries < 10) {
        setTimeout(addLogo, 100)
      }
    } else {
      document.querySelectorAll('.remark-slide-content:not(.inverse):not(.hide_logo)')
        .forEach(function (slide) {
          const logo = document.createElement('div')
          logo.classList = 'xaringan-extra-logo'
          logo.href = null
          slide.appendChild(logo)
        })
    }
  }
  document.addEventListener('DOMContentLoaded', addLogo)
})()</script>
</div>




<style>.panelset{--panel-tab-background: #F8F8F8;--panel-tab-active-background: #F8F8F8;--panel-tab-hover-background: #F8F8F8;}</style>

<style>.xe__progress-bar__container {
  top:0;
  opacity: 1;
  position:absolute;
  right:0;
  left: 0;
}
.xe__progress-bar {
  height: 0.25em;
  background-color: #F8F8F8;
  width: calc(var(--slide-current) / var(--slide-total) * 100%);
}
.remark-visible .xe__progress-bar {
  animation: xe__progress-bar__wipe 200ms forwards;
  animation-timing-function: cubic-bezier(.86,0,.07,1);
}
@keyframes xe__progress-bar__wipe {
  0% { width: calc(var(--slide-previous) / var(--slide-total) * 100%); }
  100% { width: calc(var(--slide-current) / var(--slide-total) * 100%); }
}</style>











---
class: middle

.pull-left[

]
.pull-right[

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

# Sign In


]
---
class: middle

## Learning Objectives

.panelset[

.panel[.panel-name[What will I learn?]

By the end of this week you will:

* Be able to identify what kinds of social science research questions cluster analysis may be useful for answering.

* Be able to explain, in basic terms, how two types of cluster analysis methods work: __k-means__ clustering and __Hierarchical Cluster Analysis__ clustering.

* Know which forms of cluster analysis are appropriate for the specific data and underlying clustering phenomenon you are interested in.

* Be able to run and inspect the output of __k-means__ and __HCA__ analyses of data in `R`

* Be able to describe the characteristics of clusters derived from __k-means__ and __HCA__ analyses.


]

.panel[.panel-name[How does this week fit into my course?]

* Cluster analysis is an increasingly important tool — both in social research itself and in understanding how data and machine learning algorithms are used in society. This week will teach you how it operates.

* Cluster analysis can be used in conjunction with regression analysis to explore interesting research questions, or to frame them in interesting ways.

* Cluster analysis can be used for the refinement of theory, particularly in the study of social ordering and social grouping. It can also be useful (along with Factor Analysis) for dealing with multicollinearity (e.g. by changing a large number of highly correlated variables into mutually distinct clustered groups).


]


]




???


---

class: inverse, middle

Week 10: Cluster Analysis — Part I
# What is cluster analysis and why learn it?


---

## What is cluster analysis?

.pull-left[

#### What if we have data but we are not interested in the relationship between variables? 

What if, instead, our theories are more about whether our observations fall into predictable groups (or "clusters")?

Alternatively, what if we think a collection of variables are really capturing a smaller number of underlying processes (or "factors")?

]


---

## What is cluster analysis?

.pull-left[

#### What if we have data but we are not interested in the relationship between variables? 

What if, instead, our theories are more about whether our observations fall into predictable groups (or "clusters")?

Alternatively, what if we think a collection of variables are really capturing a smaller number of underlying processes (or "factors")?

]

.pull-right[

#### For example...

What if we want to test a theory that most people fall into two 'types' of working patterns (e.g. "sprinters", "slow-and-steadys")?

Or that learners fall into four distinct groups (e.g. "visual-learners", "auditory-learners", "kinesthetic learners", "mixed learners")?

Or that the concept of 'introversion' could be measured by questions about the places a person prefers spending their free time, what their idea of a dream holiday is, and the extent to which they find social events draining or energizing?

]

---

## What is cluster analysis?

.pull-left[

#### What if we have data but we are not interested in the relationship between variables? 

What if, instead, our theories are more about whether our observations fall into predictable groups (or "clusters")?

Alternatively, what if we think a collection of variables are really capturing a smaller number of underlying processes (or "factors")?

]

.pull-right[

#### Cluster Analysis

__Cluster analysis__ refers to a collection of methods designed to __identify underlying group membership__ or __hierarchical structures__ in data, based on __similarities between observations__.


#### Factor Analysis

__Factor analysis__ refers to a collection of methods designed to __capture and approximate an underlying construct that cannot be measured directly__, but can be approximated based on __similarities between multiple variables__. 

]


---

## What is cluster analysis?

.pull-left[

#### What if we have data but we are not interested in the relationship between variables? 

What if, instead, our theories are more about whether our observations fall into predictable groups (or "clusters")?

Alternatively, what if we think a collection of variables are really capturing a smaller number of underlying processes (or "factors")?

]

.pull-right[

#### Cluster Analysis

"Learners fall into four distinct groups (e.g. "visual-learners", "auditory-learners", "kinesthetic learners", "mixed learners")."

__Variables__ might be: Learner's rating of a visual-focused teaching class (0-100), Learner's rating of a auditory-focused teaching class (0-100), Learner's rating of a kinesthetic-focused teaching class (0-100), Learner's rating of a mixed-style teaching class (0-100).

Hypothesis: Learners __cluster__ into three groups characterised by higher ratings of a single style, or a fourth group characterised by higher ratings of the mixed-style.


]


---

## What is cluster analysis?

.pull-left[

#### What if we have data but we are not interested in the relationship between variables? 

What if, instead, our theories are more about whether our observations fall into predictable groups (or "clusters")?

Alternatively, what if we think a collection of variables are really capturing a smaller number of underlying processes (or "factors")?

]

.pull-right[

#### Factor Analysis

"The concept of 'introversion' could be measured by questions about the places a person prefers spending their free time, what their idea of a dream holiday is, and the extent to which they find social events draining or energizing"

__Variables__ might be: How much do you agree with the following statements: I prefer to spend my free time at home, rather than out and about (Strong Disagree - Strong Agree); My dream holiday would involve lots of activities, new experiences, and time socialising (Strong Disagree - Strong Agree); I find social events (birthdays, holidays, etc) energizing (Strong Disagree - Strong Agree)

Hypothesis: Reponses to the above variables correspond strongly with an __underlying factor__ that can be called "introversion".


]

---

class: middle

#### For this module, we will only be able to cover two methods of __cluster analysis__; however, you may find factor analysis an interesting subject (especially if you are interested in statistical measurement, e.g. psychometrics)



---

class: inverse, middle

Week 10: Cluster Analysis — Part II
# k-means: What is it and how does it work?


---

## The k-means clustering algorithm: how does it work?

.pull-left[

* k-means is a clustering algorithm that __aims to group together observations within `\(k\)` number of groups__. The researcher selects the number of groups and needs to decide how many should be used in the end.

]

.pull-right[



&lt;img src="week-10-cluster-analysis_files/figure-html/unnamed-chunk-2-1.png" width="500" height="500" /&gt;

]

---

## The k-means clustering algorithm: how does it work?

.pull-left[

* k-means starts by generating `\(k\)` random '__centroids__'. It places these in random places in the data (indicated by circles with Xs in them). 

]


.pull-right[

&lt;img src="week-10-cluster-analysis_files/figure-html/unnamed-chunk-3-1.png" width="500" height="500" /&gt;


]

---

## The k-means clustering algorithm: how does it work?

.pull-left[

* It then calculates the distance between all of the points and the `\(k\)` centroids. It then assigns each observation to its closest centroid.

]


.pull-right[

&lt;img src="week-10-cluster-analysis_files/figure-html/unnamed-chunk-4-1.png" width="500" height="500" /&gt;


]

---

## The k-means clustering algorithm: how does it work?

.pull-left[

* The algorithm then calculates the mean values of all variables for all of the points assigned to each centroid. It then assigns a new position to the centroid at the mean position of all of the points.

]


.pull-right[

&lt;img src="week-10-cluster-analysis_files/figure-html/unnamed-chunk-5-1.png" width="500" height="500" /&gt;


]

---

## The k-means clustering algorithm: how does it work?

.pull-left[

* The algorithm reassigns all of the points to their new closest centroid (if the closest centroid has changed). 

]


.pull-right[

&lt;img src="week-10-cluster-analysis_files/figure-html/unnamed-chunk-6-1.png" width="500" height="500" /&gt;


]

---

## The k-means clustering algorithm: how does it work?

.pull-left[

* The k-means algorithm then repeats the process: it calculates the mean values of all the points assigned to the centroids and moves their position according to this new value.

]


.pull-right[

&lt;img src="week-10-cluster-analysis_files/figure-html/unnamed-chunk-7-1.png" width="500" height="500" /&gt;


]


---

## The k-means clustering algorithm: how does it work?

.pull-left[

* It then reassigns the points again...

]


.pull-right[

&lt;img src="week-10-cluster-analysis_files/figure-html/unnamed-chunk-8-1.png" width="500" height="500" /&gt;


]

---

## The k-means clustering algorithm: how does it work?

.pull-left[

* And repeats the process of moving the centroids to the mean of their assigned points.

]


.pull-right[

&lt;img src="week-10-cluster-analysis_files/figure-html/unnamed-chunk-9-1.png" width="500" height="500" /&gt;


]


---

## The k-means clustering algorithm: how does it work?

.pull-left[

* It does this until the points are no longer being reassigned between centroids (__convergence__), or, more preceisely, until no movement of the centroids makes the sum of the distance between them and their assigned points any smaller.

]


.pull-right[

&lt;img src="week-10-cluster-analysis_files/figure-html/unnamed-chunk-10-1.png" width="500" height="500" /&gt;


]



---

class: inverse, middle

Week 10: Cluster Analysis — Part III
# k-means: What kinds of people visit museums?


---

## k-means cluster analysis in `R`

.pull-left-small[

#### Do patrons of museums fall into clearly defined clusters?

This (simulated) data is about 300 visitors to a museum. The data includes variables about number of adults the person visited with; the number of children they visited with; the donation they gave (in ££.pp); their age; the amount they spent at the gift shop; and the amount they spent at the cafe.

]

.pull-right-big[


```r
head(museum_data, 5)
```

```
## # A tibble: 5 × 7
##   museum              nadults nkids donation   age giftshop_spend cafe_spend
##   &lt;chr&gt;                 &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;
## 1 Western Park Museum       2     0       35    71           6.21       9.98
## 2 Western Park Museum       3     0       45    63           5.99      12.7 
## 3 Western Park Museum       3     0       40    66           6.54      13.6 
## 4 Western Park Museum       2     1       35    68           5.29      10.6 
## 5 Western Park Museum       1     0       30    67           5.51      14.9
```


```r
stargazer::stargazer(as.data.frame(museum_data), header = FALSE, type = "html")
```


&lt;table style="text-align:center"&gt;&lt;tr&gt;&lt;td colspan="6" style="border-bottom: 1px solid black"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="text-align:left"&gt;Statistic&lt;/td&gt;&lt;td&gt;N&lt;/td&gt;&lt;td&gt;Mean&lt;/td&gt;&lt;td&gt;St. Dev.&lt;/td&gt;&lt;td&gt;Min&lt;/td&gt;&lt;td&gt;Max&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan="6" style="border-bottom: 1px solid black"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="text-align:left"&gt;nadults&lt;/td&gt;&lt;td&gt;300&lt;/td&gt;&lt;td&gt;2.617&lt;/td&gt;&lt;td&gt;1.203&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;nkids&lt;/td&gt;&lt;td&gt;300&lt;/td&gt;&lt;td&gt;0.953&lt;/td&gt;&lt;td&gt;1.409&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;donation&lt;/td&gt;&lt;td&gt;300&lt;/td&gt;&lt;td&gt;17.833&lt;/td&gt;&lt;td&gt;17.285&lt;/td&gt;&lt;td&gt;0.600&lt;/td&gt;&lt;td&gt;65.000&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;age&lt;/td&gt;&lt;td&gt;300&lt;/td&gt;&lt;td&gt;41.187&lt;/td&gt;&lt;td&gt;21.452&lt;/td&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;83&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;giftshop_spend&lt;/td&gt;&lt;td&gt;300&lt;/td&gt;&lt;td&gt;13.290&lt;/td&gt;&lt;td&gt;14.256&lt;/td&gt;&lt;td&gt;0.710&lt;/td&gt;&lt;td&gt;44.260&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;cafe_spend&lt;/td&gt;&lt;td&gt;300&lt;/td&gt;&lt;td&gt;6.313&lt;/td&gt;&lt;td&gt;5.015&lt;/td&gt;&lt;td&gt;1.400&lt;/td&gt;&lt;td&gt;21.060&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan="6" style="border-bottom: 1px solid black"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

]

---

## k-means cluster analysis in `R`

.pull-left[

* Prepare our data.

* Figure out the appropriate number of clusters for our data.

* Use k-means to cluster our data.

* Visualise the clusters (and their uniqueness).

* Add the cluster membership to our data.

* Describe the characteristics of each cluster.

]

.pull-right[

We are going to use the `factoextra` and `cluster` packages to help us run our cluster analysis.


```r
# install.packages("factoextra")
# install.packages("cluster")
library(factoextra)
library(cluster)
```

]


---

## k-means cluster analysis in `R`

.pull-left[

* __Prepare our data.__

k-means will only work with data that includes __only__ numeric variables (and all of these variables should be continuous). Therefore, we need to start by creating a subset of our data of __just__ the numeric variables we want to cluster on. 

* .grey[Figure out the appropriate number of clusters for our data.]

* .grey[Use k-means to cluster our data.]

* .grey[Visualise the clusters (and their uniqueness).]

* .grey[Add the cluster membership to our data.]

* .grey[Describe the characteristics of each cluster.]

]

.pull-right[



```r
museum_data_prepped &lt;- museum_data %&gt;%
  select(nadults, nkids, donation,
         age, giftshop_spend, cafe_spend)

museum_data_prepped
```

```
## # A tibble: 300 × 6
##    nadults nkids donation   age giftshop_spend cafe_spend
##      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;
##  1       2     0       35    71           6.21       9.98
##  2       3     0       45    63           5.99      12.7 
##  3       3     0       40    66           6.54      13.6 
##  4       2     1       35    68           5.29      10.6 
##  5       1     0       30    67           5.51      14.9 
##  6       3     0       40    69           4.46       7.36
##  7       1     0       40    65           4.33      12.6 
##  8       2     0       15    72           5.68      14.5 
##  9       5     0       35    70           5.19      13.5 
## 10       4     0       45    67           3.61      12.2 
## # ℹ 290 more rows
```

]

---

background-color: white

## k-means cluster analysis in `R`

.pull-left[

* .grey[Prepare our data.]

* __Figure out the appropriate number of clusters for our data.__

Sometimes, we have a pre-defined hypothesis of how many clusters there are in our data, but often we do not. We can use a silhouette or gap statistic plot, which tells us __which number of clusters best maximises the clustering of our data__. 

The highest silhouette value reflects the optimal number of clusters under this definition.

* .grey[Use k-means to cluster our data.]

* .grey[Visualise the clusters (and their uniqueness).]

* .grey[Add the cluster membership to our data.]

* .grey[Describe the characteristics of each cluster.]

]

.pull-right[



```r
factoextra::fviz_nbclust(museum_data_prepped, 
                         method = "silhouette",
                         FUNcluster = kmeans)
```

&lt;img src="week-10-cluster-analysis_files/figure-html/unnamed-chunk-15-1.png" width="500" height="400" /&gt;

]

---

background-color: white

## k-means cluster analysis in `R`

.pull-left[

* .grey[Prepare our data.]

* __Figure out the appropriate number of clusters for our data.__

Sometimes, we have a pre-defined hypothesis of how many clusters there are in our data, but often we do not. We can use a silhouette or gap statistic plot, which tells us __which number of clusters best maximises the clustering of our data__. 

The highest gap statistic __above a margin of error__ tells us the best solution under this condition.

* .grey[Use k-means to cluster our data.]

* .grey[Visualise the clusters (and their uniqueness).]

* .grey[Add the cluster membership to our data.]

* .grey[Describe the characteristics of each cluster.]

]

.pull-right[



```r
factoextra::fviz_nbclust(museum_data_prepped, 
                         method = "gap",
                         FUNcluster = kmeans, verbose = FALSE)
```

&lt;img src="week-10-cluster-analysis_files/figure-html/unnamed-chunk-16-1.png" width="500" height="400" /&gt;

]

---

background-color: white

## k-means cluster analysis in `R`

.pull-left[

* .grey[Prepare our data.]

* __Figure out the appropriate number of clusters for our data.__

Our silhouette and gap statistics seem to be indicating that either a __3__ or __4__ cluster solution would be a good fit to our data.

* .grey[Use k-means to cluster our data.]

* .grey[Visualise the clusters (and their uniqueness).]

* .grey[Add the cluster membership to our data.]

* .grey[Describe the characteristics of each cluster.]

]

.pull-right[



```r
factoextra::fviz_nbclust(museum_data_prepped, 
                         method = "gap",
                         FUNcluster = kmeans, verbose = FALSE)
```

&lt;img src="week-10-cluster-analysis_files/figure-html/unnamed-chunk-17-1.png" width="500" height="400" /&gt;

]


---

## k-means cluster analysis in `R`

.pull-left[

* .grey[Prepare our data.]

* .grey[Figure out the appropriate number of clusters for our data.]

* __Use k-means to cluster our data.__

Now we know an appropriate number of clusters, we can create both 3-cluster and 4-cluster solutions for our data using the `kmeans()` function. `centers = 3` is for 3 clusters, `centers = 4` for 4 clusters, etc.

* .grey[Visualise the clusters (and their uniqueness).]

* .grey[Add the cluster membership to our data.]

* .grey[Describe the characteristics of each cluster.]

]

.pull-right[



```r
set.seed(2021)
museum_3k &lt;- kmeans(museum_data_prepped, 
                    centers = 3)
set.seed(2021)
museum_4k &lt;- kmeans(museum_data_prepped, 
                    centers = 4)
```

]

---

## k-means cluster analysis in `R`

.pull-left[

* .grey[Prepare our data.]

* .grey[Figure out the appropriate number of clusters for our data.]

* __Use k-means to cluster our data.__

Now we know an appropriate number of clusters, we can create both 3-cluster and 4-cluster solutions for our data using the `kmeans()` function. `centers = 3` is for 3 clusters, `centers = 4` for 4 clusters, etc.

If we inspect the k-means objects, we get the following information.

* .grey[Visualise the clusters (and their uniqueness).]

* .grey[Add the cluster membership to our data.]

* .grey[Describe the characteristics of each cluster.]

]

.pull-right[



```r
museum_3k
```

```
## K-means clustering with 3 clusters of sizes 100, 100, 100
## 
## Cluster means:
##   nadults nkids donation   age giftshop_spend cafe_spend
## 1    1.88  2.61   10.320 33.56        33.0292     2.9782
## 2    3.38  0.05    3.028 19.97         1.9467     3.0658
## 3    2.59  0.20   40.150 70.03         4.8955    12.8940
## 
## Clustering vector:
##   [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
##  [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
##  [75] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1
## [112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
## [149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
## [186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [223] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [260] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [297] 2 2 2 2
## 
## Within cluster sum of squares by cluster:
## [1]  3608.9634   370.0866 14963.0429
##  (between_SS / total_SS =  93.6 %)
## 
## Available components:
## 
## [1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
## [6] "betweenss"    "size"         "iter"         "ifault"
```

]


---

## k-means cluster analysis in `R`

.pull-left[

* .grey[Prepare our data.]

* .grey[Figure out the appropriate number of clusters for our data.]

* __Use k-means to cluster our data.__

1. We see descriptive statistics for each of our clusters (we will revisit these by adding them to our data)

2. We see the cluster assigned to each observation

3. We see the proportion of variance that can be explained by cluster membership (between_SS/total_SS) (93.6%)

* .grey[Visualise the clusters (and their uniqueness).]

* .grey[Add the cluster membership to our data.]

* .grey[Describe the characteristics of each cluster.]

]

.pull-right[



```r
museum_3k
```

```
## K-means clustering with 3 clusters of sizes 100, 100, 100
## 
## Cluster means:
##   nadults nkids donation   age giftshop_spend cafe_spend
## 1    1.88  2.61   10.320 33.56        33.0292     2.9782
## 2    3.38  0.05    3.028 19.97         1.9467     3.0658
## 3    2.59  0.20   40.150 70.03         4.8955    12.8940
## 
## Clustering vector:
##   [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
##  [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
##  [75] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1
## [112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
## [149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
## [186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [223] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [260] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [297] 2 2 2 2
## 
## Within cluster sum of squares by cluster:
## [1]  3608.9634   370.0866 14963.0429
##  (between_SS / total_SS =  93.6 %)
## 
## Available components:
## 
## [1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
## [6] "betweenss"    "size"         "iter"         "ifault"
```

]

---

## k-means cluster analysis in `R`

.pull-left[

* .grey[Prepare our data.]

* .grey[Figure out the appropriate number of clusters for our data.]

* __Use k-means to cluster our data.__

We can call specific parts of the k-means object using the `$` operator.

* .grey[Visualise the clusters (and their uniqueness).]

* .grey[Add the cluster membership to our data.]

* .grey[Describe the characteristics of each cluster.]

]

.pull-right[



```r
museum_3k$cluster
```

```
##   [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
##  [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
##  [75] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1
## [112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
## [149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
## [186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [223] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [260] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [297] 2 2 2 2
```

```r
museum_3k$centers
```

```
##   nadults nkids donation   age giftshop_spend cafe_spend
## 1    1.88  2.61   10.320 33.56        33.0292     2.9782
## 2    3.38  0.05    3.028 19.97         1.9467     3.0658
## 3    2.59  0.20   40.150 70.03         4.8955    12.8940
```

]


---

background-color: white

## k-means cluster analysis in `R`

.pull-left[

* .grey[Prepare our data.]

* .grey[Figure out the appropriate number of clusters for our data.]

* .grey[Use k-means to cluster our data.]

* __Visualise the clusters (and their uniqueness).__

When we have two or more possible solutions to the number of clusters it can be helpful to visualise them with a __biplot__.

A biplot uses something called Principal Components Analysis to simplify multiple dimensions of data (multiple variables) down to two dimensions. We can then plot the observations and their cluster membership using `factoextra::fviz_cluster`. 


* .grey[Add the cluster membership to our data.]

* .grey[Describe the characteristics of each cluster.]

]

.pull-right[



```r
fviz_cluster(museum_3k, data = museum_data_prepped,
             geom = "point") +
  ggthemes::scale_color_colorblind()
```

&lt;img src="week-10-cluster-analysis_files/figure-html/unnamed-chunk-22-1.png" width="500" height="400" /&gt;

]

---

background-color: white

## k-means cluster analysis in `R`

.pull-left[

* .grey[Prepare our data.]

* .grey[Figure out the appropriate number of clusters for our data.]

* .grey[Use k-means to cluster our data.]

* __Visualise the clusters (and their uniqueness).__

This creates a ggplot type plot that we can customise, e.g. with colourblind friendly scales from the `ggthemes` package.

Does the four cluster solution look much better than the three cluster solution?


* .grey[Add the cluster membership to our data.]

* .grey[Describe the characteristics of each cluster.]

]

.pull-right[



```r
fviz_cluster(museum_4k, data = museum_data_prepped,
             geom = "point") +
  ggthemes::scale_color_colorblind()
```

&lt;img src="week-10-cluster-analysis_files/figure-html/unnamed-chunk-23-1.png" width="500" height="400" /&gt;

]

---

## k-means cluster analysis in `R`

.pull-left[

* .grey[Prepare our data.]

* .grey[Figure out the appropriate number of clusters for our data.]

* .grey[Use k-means to cluster our data.]

* .grey[Visualise the clusters (and their uniqueness).]

* __Add the cluster membership to our data.__

We can add the cluster membership to our data easily using the object from kmeans and the `dplyr` package from `tidyverse`.

* .grey[Describe the characteristics of each cluster.]

]

.pull-right[



```r
museum_data &lt;- museum_data %&gt;%
  mutate(
    cluster = museum_3k$cluster
  )

museum_data
```

```
## # A tibble: 300 × 8
##    museum         nadults nkids donation   age giftshop_spend cafe_spend cluster
##    &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;   &lt;int&gt;
##  1 Western Park …       2     0       35    71           6.21       9.98       3
##  2 Western Park …       3     0       45    63           5.99      12.7        3
##  3 Western Park …       3     0       40    66           6.54      13.6        3
##  4 Western Park …       2     1       35    68           5.29      10.6        3
##  5 Western Park …       1     0       30    67           5.51      14.9        3
##  6 Western Park …       3     0       40    69           4.46       7.36       3
##  7 Western Park …       1     0       40    65           4.33      12.6        3
##  8 Western Park …       2     0       15    72           5.68      14.5        3
##  9 Western Park …       5     0       35    70           5.19      13.5        3
## 10 Western Park …       4     0       45    67           3.61      12.2        3
## # ℹ 290 more rows
```

]


---

## k-means cluster analysis in `R`

.pull-left[

* .grey[Prepare our data.]

* .grey[Figure out the appropriate number of clusters for our data.]

* .grey[Use k-means to cluster our data.]

* .grey[Visualise the clusters (and their uniqueness).]

* .grey[Add the cluster membership to our data.]

* __Describe the characteristics of each cluster.__

We can then carry out any bivariate statistics, tests, or other analyses with both variables that were in the cluster process and those that were not to better understand out clusters.

]

.pull-right[



```r
museum_data %&gt;%
  group_by(cluster) %&gt;%
  summarise(
    m_nadults = mean(nadults),
    m_nkids = mean(nkids),
    m_donation = mean(donation),
    m_age = mean(age),
    m_giftshop = mean(giftshop_spend),
    m_cafe = mean(cafe_spend)
  )
```

```
## # A tibble: 3 × 7
##   cluster m_nadults m_nkids m_donation m_age m_giftshop m_cafe
##     &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;
## 1       1      1.88    2.61      10.3   33.6      33.0    2.98
## 2       2      3.38    0.05       3.03  20.0       1.95   3.07
## 3       3      2.59    0.2       40.2   70.0       4.90  12.9
```

]

---

background-color: white

## k-means cluster analysis in `R`

.pull-left[

* .grey[Prepare our data.]

* .grey[Figure out the appropriate number of clusters for our data.]

* .grey[Use k-means to cluster our data.]

* .grey[Visualise the clusters (and their uniqueness).]

* .grey[Add the cluster membership to our data.]

* __Describe the characteristics of each cluster.__

[Join the Wooclap (Week 10: Wooclap 1)](https://app.wooclap.com/events/SMIGJA/questions/66e84343e4f5d5ad4d599f0c) using this [link](https://app.wooclap.com/events/SMIGJA/questions/66e84343e4f5d5ad4d599f0c) (Event ID: **SMIGJA**) and try interpreting the clusters produced by k-means, based on their mean values.

]

.pull-right[

&lt;center&gt;
&lt;img src="images/week-10-wooclap-1.png" width="80%" /&gt;
&lt;/center&gt;

]






---

## k-means cluster analysis in `R`

.pull-left[

* .grey[Prepare our data.]

* .grey[Figure out the appropriate number of clusters for our data.]

* .grey[Use k-means to cluster our data.]

* .grey[Visualise the clusters (and their uniqueness).]

* .grey[Add the cluster membership to our data.]

* __Describe the characteristics of each cluster.__

e.g. 1 = Around 2 adults, large number of children, average donation, age 30s, high gift shop spend, low cafe spend... __Families with kids bringing packed lunches and buying mementos__?

]

.pull-right[



```r
museum_data %&gt;%
  group_by(cluster) %&gt;%
  summarise(
    m_nadults = mean(nadults),
    m_nkids = mean(nkids),
    m_donation = mean(donation),
    m_age = mean(age),
    m_giftshop = mean(giftshop_spend),
    m_cafe = mean(cafe_spend)
  )
```

```
## # A tibble: 3 × 7
##   cluster m_nadults m_nkids m_donation m_age m_giftshop m_cafe
##     &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;
## 1       1      1.88    2.61      10.3   33.6      33.0    2.98
## 2       2      3.38    0.05       3.03  20.0       1.95   3.07
## 3       3      2.59    0.2       40.2   70.0       4.90  12.9
```

]

---

## k-means cluster analysis in `R`

.pull-left[

* .grey[Prepare our data.]

* .grey[Figure out the appropriate number of clusters for our data.]

* .grey[Use k-means to cluster our data.]

* .grey[Visualise the clusters (and their uniqueness).]

* .grey[Add the cluster membership to our data.]

* __Describe the characteristics of each cluster.__

e.g. 2 = Larger number of adults, very few if ever kids, low donations, overall quite young (around 20), low giftshop spend, low cafe spend... __Groups of students killing time, on a field trip, or stopping for a coffee__?

]

.pull-right[



```r
museum_data %&gt;%
  group_by(cluster) %&gt;%
  summarise(
    m_nadults = mean(nadults),
    m_nkids = mean(nkids),
    m_donation = mean(donation),
    m_age = mean(age),
    m_giftshop = mean(giftshop_spend),
    m_cafe = mean(cafe_spend)
  )
```

```
## # A tibble: 3 × 7
##   cluster m_nadults m_nkids m_donation m_age m_giftshop m_cafe
##     &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;
## 1       1      1.88    2.61      10.3   33.6      33.0    2.98
## 2       2      3.38    0.05       3.03  20.0       1.95   3.07
## 3       3      2.59    0.2       40.2   70.0       4.90  12.9
```

]

---

## k-means cluster analysis in `R`

.pull-left[

* .grey[Prepare our data.]

* .grey[Figure out the appropriate number of clusters for our data.]

* .grey[Use k-means to cluster our data.]

* .grey[Visualise the clusters (and their uniqueness).]

* .grey[Add the cluster membership to our data.]

* __Describe the characteristics of each cluster.__

e.g. 3 = Often 2, sometimes 3 adults; sometimes children, but not often; average very high donation; very high average age (around 70 years); modest giftshop spend, and high cafe spend... __retired patrons of the museum who like to have a full lunch at the cafe__?

]

.pull-right[



```r
museum_data %&gt;%
  group_by(cluster) %&gt;%
  summarise(
    m_nadults = mean(nadults),
    m_nkids = mean(nkids),
    m_donation = mean(donation),
    m_age = mean(age),
    m_giftshop = mean(giftshop_spend),
    m_cafe = mean(cafe_spend)
  )
```

```
## # A tibble: 3 × 7
##   cluster m_nadults m_nkids m_donation m_age m_giftshop m_cafe
##     &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;
## 1       1      1.88    2.61      10.3   33.6      33.0    2.98
## 2       2      3.38    0.05       3.03  20.0       1.95   3.07
## 3       3      2.59    0.2       40.2   70.0       4.90  12.9
```

]


---

## k-means cluster analysis in `R`

.pull-left[

* .grey[Prepare our data.]

* .grey[Figure out the appropriate number of clusters for our data.]

* .grey[Use k-means to cluster our data.]

* .grey[Visualise the clusters (and their uniqueness).]

* .grey[Add the cluster membership to our data.]

* __Describe the characteristics of each cluster.__

It's quite common to give either straightforward or amusing names to the clusters, based on their characteristics. You will often see this in Facebook/Spotify/etc. website metadata as a way to target ads! 

]

.pull-right[



```r
museum_data %&gt;%
  group_by(cluster) %&gt;%
  summarise(
    m_nadults = mean(nadults),
    m_nkids = mean(nkids),
    m_donation = mean(donation),
    m_age = mean(age),
    m_giftshop = mean(giftshop_spend),
    m_cafe = mean(cafe_spend)
  ) %&gt;%
  mutate(
    cluster = case_when(cluster == 1 ~ "Young Families",
                        cluster == 2 ~ "Student Groups",
                        cluster == 3 ~ "High-rolling Oldies")
  )
```

```
## # A tibble: 3 × 7
##   cluster             m_nadults m_nkids m_donation m_age m_giftshop m_cafe
##   &lt;chr&gt;                   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;
## 1 Young Families           1.88    2.61      10.3   33.6      33.0    2.98
## 2 Student Groups           3.38    0.05       3.03  20.0       1.95   3.07
## 3 High-rolling Oldies      2.59    0.2       40.2   70.0       4.90  12.9
```

]



---

class: inverse, middle

Week 10: Cluster Analysis — Part IV
# k-means: Assumptions.


---

## k-means assumptions

.pull-left[

k-means does not quite have the same kinds of 'assumptions' as statistical models like regression, t-tests, etc., but it does make some assumptions about the data and it does have some requirements.

#### Data must be continuous

It is not recommended to use k-means with binary or other forms of data, though pseudo-continuous variables (e.g. likert scales) may be fine.

]

.pull-right[

]

---

## k-means assumptions

.pull-left[

k-means does not quite have the same kinds of 'assumptions' as statistical models like regression, t-tests, etc., but it does make some assumptions about the data and it does have some requirements.

#### Data must be continuous

It is not recommended to use k-means with binary or other forms of data, though pseudo-continuous variables (e.g. likert scales) may be fine.

]

.pull-right[

#### Clusters must be (approximately) spherical, of similar size, and with similar variance

k-means also assumes that the best fitting clusters to your data are approximately spherical (e.g. not oblong, or any other weird shape). This can be checked using your biplot (to see where misclassifications may be occuring). There are some methods designed to handle non-spherical clustering.

]

---

background-color: white

## k-means assumptions

.pull-left[
.center[
&lt;img src="images/kmeans-assumptions.png" width="450" height="450" /&gt;
]


Source: [Morbieu, 2018](https://smorbieu.gitlab.io/k-means-is-not-all-about-sunshines-and-rainbows/)

]

.pull-right[

#### Clusters must be (approximately) spherical, of similar size, and with similar variance

k-means also assumes that the best fitting clusters to your data are approximately spherical (e.g. not oblong, or any other weird shape). This can be checked using your biplot (to see where misclassifications may be occuring). There are some methods designed to handle non-spherical clustering.

]


---

class: hide_logo, middle
background-image: url("images/ac-bg.png")
background-size: contain

.center[
&lt;img src="images/ac-hclust.png" width="60%" /&gt;
]

---

class: inverse, middle

Week 10: Cluster Analysis — Part V
# Hierarchical Cluster Analysis: How does it work?


---

background-color: white

## Hierarchical Cluster Analysis

.pull-left-big[

&lt;br&gt;

&lt;img src="images/hca-how.png" width="100%" /&gt;

]

.pull-right-small[



]


---

background-color: white

## Hierarchical Cluster Analysis

.pull-left-big[

&lt;br&gt;

&lt;img src="images/hca-how.png" width="100%" /&gt;

]

.pull-right-small[

* __Calculate an appropriate distance (dissimilarity) measure between all points__ in a matrix.


]

---

background-color: white

## Hierarchical Cluster Analysis

.pull-left-big[

&lt;br&gt;

&lt;img src="images/hca-how.png" width="100%" /&gt;

]

.pull-right-small[

* __Calculate an appropriate distance (dissimilarity) measure between all points__ in a matrix.

* Use one of several algorithms to __agglomoratively__ or __divisively__ link all points, depending on the theoretical type of clusters.


]

---

background-color: white

## Hierarchical Cluster Analysis

.pull-left-big[

&lt;br&gt;

&lt;img src="images/hca-how.png" width="100%" /&gt;

]

.pull-right-small[

* __Calculate an appropriate distance (dissimilarity) measure between all points__ in a matrix.

* Use one of several algorithms to __agglomoratively__ or __divisively__ link all points, depending on the theoretical type of clusters.

* Visualise result in a __dendrogram__ and decide on (one or more) solutions.


]


---

background-color: white

## Hierarchical Cluster Analysis

.pull-left-big[

&lt;br&gt;

&lt;img src="images/hca-how.png" width="100%" /&gt;

]

.pull-right-small[

* __Calculate an appropriate distance (dissimilarity) measure between all points__ in a matrix.

* Use one of several algorithms to __agglomoratively__ or __divisively__ link all points, depending on the theoretical type of clusters.

* Visualise result in a __dendrogram__ and decide on (one or more) solutions.

* __"Cut" tree__ to assign observations to clusters, and then describe clusters with bivariate statistics.


]


---

class: middle

## Hierarchical Cluster Analysis

#### Pros and Cons compared to k-means

.pull-left[

#### Pros

* Highly flexible — range of methods means that non-spherical, unusual, types of clustering (e.g. single point of power causing a 'chain' effect; identifying a common "type"; 'spheres of influence' that diffuse gradually; 'tight', highly defined clusters; or general unity). Often better defines strangely shaped clusters.

]

.pull-right[

#### Cons

* High degree of subjectivity in choice of hierarchical clustering algorithm, as well as choice of distance matrix and final number of clusters. No straightforward statistics for deciding on optimal number of clusters.

* Can be harder to work with, visualise nicely, etc, without more programming knowledge.

]




---

class: inverse, middle

Week 10: Cluster Analysis — Part VI
# Hierarchical Cluster Analysis: What groups do Youtube viewers fall into?


---

## HCA: Example

.pull-left-small[

#### Can we find a general typology of Youtube viewers?

We collected a random sample of videos watched on 200 Youtube users and then recorded which types of videos they had watched in the random sample of 10 that was collected for each viewer. If the person watched a video of the genre listed, a 1 was recorded. If they did not watch a video of that genre, a 0 was recorded.

]

.pull-right-big[




```r
youtube_data
```

```
## # A tibble: 200 × 12
##    animation comedy documentaries music news  reviews sport tutorials trailers
##    &lt;fct&gt;     &lt;fct&gt;  &lt;fct&gt;         &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;   &lt;fct&gt; &lt;fct&gt;     &lt;fct&gt;   
##  1 0         1      0             0     0     0       0     0         0       
##  2 1         0      0             0     0     1       0     0         1       
##  3 1         0      0             1     0     0       0     1         1       
##  4 1         0      0             1     0     1       0     0         0       
##  5 1         0      0             1     0     0       0     1         1       
##  6 1         0      0             1     0     0       0     0         1       
##  7 1         0      0             1     0     1       0     1         1       
##  8 1         1      0             1     0     0       0     1         0       
##  9 1         0      1             1     0     1       0     0         0       
## 10 0         1      0             1     0     0       0     0         1       
## # ℹ 190 more rows
## # ℹ 3 more variables: videoessays &lt;fct&gt;, videogames &lt;fct&gt;, vlogs &lt;fct&gt;
```

]


---

## HCA: Example

.pull-left[

#### Step 1: Select the most appropriate dissimilarity/distance calculation.

* __Euclidean__ distance: Distance calculated as a straight line ("as the bird flies"). Most appropriate form of distance when all variables are continuous.

* __Manhattan__ distance: Distance calculated as steps of travel (like navigating a city via blocks). Most appropriate form when we have a large number of continuous variables and/or continuous variables with very different scales, variance, or mixtures of ratio, discrete, and interval.

* __Gower__ distance: Uses a range of distance measures depending on variable type. Most appropriate when you have all categorical binary, ordinal, or mixtures of categorical, binary, and continuous data.

]


---

## HCA: Example

.pull-left[

#### Step 1: Select the most appropriate dissimilarity/distance calculation.

* __Euclidean__ distance: Distance calculated as a straight line ("as the bird flies"). Most appropriate form of distance when all variables are continuous.

* __Manhattan__ distance: Distance calculated as steps of travel (like navigating a city via blocks). Most appropriate form when we have a large number of continuous variables and/or continuous variables with very different scales, variance, or mixtures of ratio, discrete, and interval.

* __Gower__ distance: Uses a range of distance measures depending on variable type. Most appropriate when you have all categorical binary, ordinal, or mixtures of categorical, binary, and continuous data.

]

.pull-right[

We can calculate a distance/dissimilarity matrix (distance between all points) using the `daisy()` function from the `cluster` package.

Binary continuous variables __must__ be coded as factors for `daisy` to calculate Gower's distance correctly.


```r
library(cluster)

# Calculate a distance matrix and save it as youtube_d
youtube_d &lt;- daisy(youtube_data, metric = "gower")
```


]

---

## HCA: Example

.pull-left[

#### Step 1: Select the most appropriate dissimilarity/distance calculation.

* __Euclidean__ distance: Distance calculated as a straight line ("as the bird flies"). Most appropriate form of distance when all variables are continuous.

* __Manhattan__ distance: Distance calculated as steps of travel (like navigating a city via blocks). Most appropriate form when we have a large number of continuous variables and/or continuous variables with very different scales, variance, or mixtures of ratio, discrete, and interval.

* __Gower__ distance: Uses a range of distance measures depending on variable type. Most appropriate when you have all categorical binary, ordinal, or mixtures of categorical, binary, and continuous data.

]

.pull-right[

#### Visualisation of Dissimilarity Matrix

You don't need to do this, I'm just illustrating what `daisy` is calculating! Darker colours = more dissimilar observations.

&lt;center&gt;
&lt;img src="week-10-cluster-analysis_files/figure-html/unnamed-chunk-41-1.png" width="380" height="380" /&gt;
&lt;/center&gt;

]


---

## HCA: Example

.pull-left[

#### Step 2: Decide on an appropriate clustering method

* __Ward__: Identifies underlying defined 'types' (e.g. species). Should only be used with Euclidean distance.

* __single__: Identifies underlying hierarchical 'chains' — strictly most similar to least.

* __complete__: Identifies underlying 'circles' (as in: runs in the same circles). Discriminates between dissimilar overall clusters as well as individuals.

* __centroid__: Identifies clusters based on the there being a clear center with dispersion in the membership. Comparable to k-means. Should only be used with Euclidean distance.

]

.pull-right[

* __median__: Same as centroid but less sensitive to outliers. Should only be used with Euclidean distance.

* __average__: Like complete but tends towards identifying clusters of approximately equal size, defined very generically (i.e. not a chain of command, or a clear center). Based on average dissimilarity of each pair/group from all other pairs/groups as clusters are formed. 

General rule of thumb: __average__ and __complete__ linkage is a good choice for any underlying theory and data.

]

---

## HCA: Example

.pull-left[

#### Step 2: Decide on an appropriate clustering method

* .grey[__Ward__: Identifies underlying defined 'types' (e.g. species). Should only be used with Euclidean distance.]

* __single__: Identifies underlying hierarchical 'chains' — strictly most similar to least.

* __complete__: Identifies underlying 'circles' (as in: runs in the same circles). Discriminates between dissimilar overall clusters as well as individuals.

* .grey[__centroid__: Identifies clusters based on the there being a clear center with dispersion in the membership. Comparable to k-means. Should only be used with Euclidean distance.]

]

.pull-right[

* .grey[__median__: Same as centroid but less sensitive to outliers. Should only be used with Euclidean distance.]

* __average__: Like complete but tends towards identifying clusters of approximately equal size, defined very generically (i.e. not a chain of command, or a clear center). Based on average dissimilarity of each pair/group from all other pairs/groups as clusters are formed. 

General rule of thumb: __average__ and __complete__ linkage is a good choice for any underlying theory and data.

* Not going to use any that require Euclidean distance

]


---

## HCA: Example

.pull-left[

#### Step 2: Decide on an appropriate clustering method

* .grey[__Ward__: Identifies underlying defined 'types' (e.g. species). Should only be used with Euclidean distance.]

* .grey[__single__: Identifies underlying hierarchical 'chains' — strictly most similar to least.]

* __complete__: Identifies underlying 'circles' (as in: runs in the same circles). Discriminates between dissimilar overall clusters as well as individuals.

* .grey[__centroid__: Identifies clusters based on the there being a clear center with dispersion in the membership. Comparable to k-means. Should only be used with Euclidean distance.]

]

.pull-right[

* .grey[__median__: Same as centroid but less sensitive to outliers. Should only be used with Euclidean distance.]

* __average__: Like complete but tends towards identifying clusters of approximately equal size, defined very generically (i.e. not a chain of command, or a clear center). Based on average dissimilarity of each pair/group from all other pairs/groups as clusters are formed. 

General rule of thumb: __average__ and __complete__ linkage is a good choice for any underlying theory and data.

* Not going to use any that require Euclidean distance
* Not going to use single as it doesn't fit underlying theory (no highly weighted individual youtube viewer)

]


---

background-color: white

## HCA: Example

.pull-left-small[

#### Step 3: Cluster data using `hclust`

* Cluster data based on __complete linkage__


```r
set.seed(2021)
hca_comp &lt;- hclust(d = youtube_d,
                   method = "complete")
```

* Cluster data based on __average linkage__


```r
set.seed(2021)
hca_avg &lt;- hclust(d = youtube_d,
                  method = "average")
```


]


---

background-color: white

## HCA: Example

.pull-left-small[

#### Step 3: Cluster data using `hclust`

* Cluster data based on __complete linkage__


```r
set.seed(2021)
hca_comp &lt;- hclust(d = youtube_d,
                   method = "complete")
```

* Cluster data based on __average linkage__


```r
set.seed(2021)
hca_avg &lt;- hclust(d = youtube_d,
                  method = "average")
```


__Complete__: Maybe 4, maybe 6 clusters?


]

.pull-right-big[

#### Step 4: Visualise HCAs using dendrograms


```r
plot(hca_comp, main = "Complete Linkage")
```

&lt;img src="week-10-cluster-analysis_files/figure-html/unnamed-chunk-46-1.png" width="700" height="380" /&gt;

]

---

background-color: white

## HCA: Example

.pull-left-small[

#### Step 3: Cluster data using `hclust`

* Cluster data based on __complete linkage__


```r
set.seed(2021)
hca_comp &lt;- hclust(d = youtube_d,
                   method = "complete")
```

* Cluster data based on __average linkage__


```r
set.seed(2021)
hca_avg &lt;- hclust(d = youtube_d,
                  method = "average")
```


__Complete__: Maybe 4, maybe 6 clusters?

__Average__: Probably 4 clusters


]

.pull-right-big[

#### Step 4: Visualise HCAs using dendrograms


```r
plot(hca_avg, main = "Average Linkage")
```

&lt;img src="week-10-cluster-analysis_files/figure-html/unnamed-chunk-49-1.png" width="700" height="380" /&gt;

]

---

background-color: white

## HCA: Example


.pull-left-small[

#### Step 5: "Cut" tree to decide on cluster membership

* The `cutree` (NB: only one "t") can be used to cut based on either height (h =) or resultant number of clusters.






]


.pull-right-big[


```r
plot(hca_avg, main = "Average Linkage")
```

&lt;img src="week-10-cluster-analysis_files/figure-html/unnamed-chunk-50-1.png" width="700" height="400" /&gt;

]

---

background-color: white

## HCA: Example


.pull-left-small[

#### Step 5: "Cut" tree to decide on cluster membership

* The `cutree` (NB: only one "t") can be used to cut based on either height (h =) or resultant number of clusters.

* e.g. `cutree(hca_avg, h = 0.4)` would cut the tree here. 




]


.pull-right-big[


```r
plot(hca_avg, main = "Average Linkage")
abline(a = 0.4, b = 0, col = "blue")
```

&lt;img src="week-10-cluster-analysis_files/figure-html/unnamed-chunk-51-1.png" width="700" height="400" /&gt;

]

---

## HCA: Example


.pull-left-small[

#### Step 5: "Cut" tree to decide on cluster membership

* The `cutree` (NB: only one "t") can be used to cut based on either height (h =) or resultant number of clusters.

* e.g. `cutree(hca_avg, h = 0.4)` would cut the tree here. 

* Usually easiest to just specify the number of groups and have `R` calculate the equivalent height, e.g. `cutree(hca_avg, k = 4)`




]


.pull-right-big[


```r
hca_avg_k4  &lt;- cutree(hca_avg, k = 4)
hca_comp_k4 &lt;- cutree(hca_comp, k = 4)
hca_comp_k6 &lt;- cutree(hca_comp, k = 6)

# Example of what you get

hca_avg_k4
```

```
##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1
##  [38] 1 1 3 1 1 1 1 1 1 1 3 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 4 4 4 4 3 4 4
##  [75] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 3 2
## [112] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [149] 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 1 3 3 3 3
## [186] 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3
```

]

---

## HCA: Example

.pull-left[

#### Step 6: Add cluster membership 

We can add the cluster membership to our data using the `mutate` function.

]

.pull-right[


```r
youtube_data_results &lt;- youtube_data %&gt;%
  mutate(
    hca_avg_k4 = hca_avg_k4,
    hca_comp_k4 = hca_comp_k4,
    hca_comp_k6 = hca_comp_k6
  )

youtube_data_results
```

```
## # A tibble: 200 × 15
##    animation comedy documentaries music news  reviews sport tutorials trailers
##    &lt;fct&gt;     &lt;fct&gt;  &lt;fct&gt;         &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;   &lt;fct&gt; &lt;fct&gt;     &lt;fct&gt;   
##  1 0         1      0             0     0     0       0     0         0       
##  2 1         0      0             0     0     1       0     0         1       
##  3 1         0      0             1     0     0       0     1         1       
##  4 1         0      0             1     0     1       0     0         0       
##  5 1         0      0             1     0     0       0     1         1       
##  6 1         0      0             1     0     0       0     0         1       
##  7 1         0      0             1     0     1       0     1         1       
##  8 1         1      0             1     0     0       0     1         0       
##  9 1         0      1             1     0     1       0     0         0       
## 10 0         1      0             1     0     0       0     0         1       
## # ℹ 190 more rows
## # ℹ 6 more variables: videoessays &lt;fct&gt;, videogames &lt;fct&gt;, vlogs &lt;fct&gt;,
## #   hca_avg_k4 &lt;int&gt;, hca_comp_k4 &lt;int&gt;, hca_comp_k6 &lt;int&gt;
```

]


---

## HCA: Example

#### Step 7: Explore how the clusters differ with bivariate statistics




```r
youtube_data_results %&gt;%
  # change my factor variables to numeric for 
  # calculating proportions
  mutate_at(vars(animation:vlogs), ~as.numeric(.)-1) %&gt;%
  group_by(hca_avg_k4) %&gt;%
  # Means for all youtube genres
  summarise_at(vars(animation:vlogs), ~mean(., na.rm = TRUE))
```

```
## # A tibble: 4 × 13
##   hca_avg_k4 animation comedy documentaries music  news reviews sport tutorials trailers videoessays videogames  vlogs
##        &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;
## 1          1    0.854  0.667         0.0208 0.688 0       0.354 0        0.583    0.542        0         0.958  0.917 
## 2          2    0.0943 0.0377        0.0189 0.868 0.358   0.774 0.151    0.283    0.981        0         0.0377 0     
## 3          3    0.118  0.392         0.0980 0.216 0.157   0.196 0.922    0.0392   0.137        0         0.392  0.0980
## 4          4    0.0625 0.229         0.917  0.125 0.938   0.25  0.146    0.938    0.0208       0.625     0.0208 0.0833
```

.center[

&lt;br&gt;

Class Activity: Join the [Wooclap Activity](https://app.wooclap.com/events/SMIGJA/questions/66e847db000342198d5906ee) and try to give appropriate labels to each 'group' of Youtube Viewers

[Link](https://app.wooclap.com/events/SMIGJA/questions/66e847db000342198d5906ee)

Event ID: **SMIGJA** (Question 2)

]

---

background-color: white

class: middle

&lt;center&gt;

&lt;img src="images/smi606-week-10-wooclap-2.png" width="60%" /&gt;

&lt;/center&gt;

---

## HCA: Example

#### Step 7: Explore how the clusters differ with bivariate statistics




```r
youtube_data_results %&gt;%
  # change my factor variables to numeric for 
  # calculating proportions
  mutate_at(vars(animation:vlogs), ~as.numeric(.)-1) %&gt;%
  group_by(hca_avg_k4) %&gt;%
  # Means for all youtube genres
  summarise_at(vars(animation:vlogs), ~mean(., na.rm = TRUE))
```

```
## # A tibble: 4 × 13
##   hca_avg_k4 animation comedy documentaries music  news reviews sport tutorials trailers videoessays videogames  vlogs
##        &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;
## 1          1    0.854  0.667         0.0208 0.688 0       0.354 0        0.583    0.542        0         0.958  0.917 
## 2          2    0.0943 0.0377        0.0189 0.868 0.358   0.774 0.151    0.283    0.981        0         0.0377 0     
## 3          3    0.118  0.392         0.0980 0.216 0.157   0.196 0.922    0.0392   0.137        0         0.392  0.0980
## 4          4    0.0625 0.229         0.917  0.125 0.938   0.25  0.146    0.938    0.0208       0.625     0.0208 0.0833
```


* __Average Linkage 4 Group__
  * group 1: High (&gt;60%) animation, comedy, music, videogames, vlogs — "Young People Youtube"?
  * group 2: High (&gt;60%) music, reviews, trailers — "Traditional Media Adjacent"?
  * group 3: High (&gt;60%) sport, and nothing else — "Sport fans"
  * group 4: High (&gt;60%) documentaries, news, tutorials, video essays — "Info-tainment seekers"?


---

## HCA: Example

#### Step 7: Explore how the clusters differ with bivariate statistics


```r
youtube_data_results %&gt;%
  # change my factor variables to numeric for 
  # calculating proportions
  mutate_at(vars(animation:vlogs), ~as.numeric(.)-1) %&gt;%
  group_by(hca_comp_k4) %&gt;%
  # Means for all youtube genres
  summarise_at(vars(animation:vlogs), ~mean(., na.rm = TRUE))
```

```
## # A tibble: 4 × 13
##   hca_comp_k4 animation comedy documentaries  music  news reviews sport tutorials trailers videoessays videogames  vlogs
##         &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;
## 1           1    0.854  0.667         0.0208 0.688  0       0.354 0        0.583    0.542        0         0.958  0.917 
## 2           2    0.125  0.0781        0.0312 0.75   0.297   0.734 0.297    0.234    0.891        0         0.0469 0.0156
## 3           3    0.0882 0.382         0.0588 0.294  0.118   0.118 0.912    0.0294   0            0         0.559  0.118 
## 4           4    0.0556 0.278         0.852  0.0926 0.907   0.222 0.222    0.852    0.0556       0.556     0.0185 0.0741
```


* __Complete Linkage 4 Group__
  * group 1: High (&gt;60%) animation, comedy, music, videogames, vlogs — "Young People Youtube"?
  * group 2: High (&gt;60%) music, reviews, trailers — "Traditional Media Adjacent"?
  * group 3: High (&gt;60%) sport, and nothing else (maybe + videogames?) — "Sport fans"
  * group 4: High (&gt;60%) documentaries, news, tutorials — "Info-tainment seekers"?

---

## HCA: Example

#### Step 7: Explore how the clusters differ with bivariate statistics


```r
youtube_data_results %&gt;%
  # change my factor variables to numeric for 
  # calculating proportions
  mutate_at(vars(animation:vlogs), ~as.numeric(.)-1) %&gt;%
  group_by(hca_comp_k6) %&gt;%
  # Means for all youtube genres
  summarise_at(vars(animation:vlogs), ~mean(., na.rm = TRUE)) %&gt;%
  print(width = Inf) 
```

```
## # A tibble: 6 × 13
##   hca_comp_k6 animation comedy documentaries  music  news reviews  sport tutorials trailers videoessays videogames  vlogs
##         &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;
## 1           1    0.854  0.667         0.0208 0.688  0       0.354 0         0.583    0.542        0         0.958  0.917 
## 2           2    0.5    0             0      1      0.667   1     0.333     1        1            0         0.167  0     
## 3           3    0.0882 0.382         0.0588 0.294  0.118   0.118 0.912     0.0294   0            0         0.559  0.118 
## 4           4    0.0556 0.278         0.852  0.0926 0.907   0.222 0.222     0.852    0.0556       0.556     0.0185 0.0741
## 5           5    0.0238 0.0476        0.0476 0.881  0.357   0.690 0.0238    0.190    0.976        0         0.0476 0     
## 6           6    0.25   0.188         0      0.312  0       0.75  1         0.0625   0.625        0         0      0.0625
```


* __Complete Linkage 6 Group__
  * group 1: High (&gt;60%) animation, comedy, music, videogames, vlogs — "Young People Youtube"?
  * group 2: High (&gt;60%) music, news, reviews, tutorials — "Traditional Media Adjacent + Guides"?
  * group 3: High (&gt;60%) sport, and nothing else (maybe + videogames?) — "Sport fans"
  * group 4: High (&gt;60%) documentaries, news, tutorials — "Info-tainment seekers"? 
  * group 5: High (&gt;60%) music, reviews, trailers — "Traditional Media Adjacent"?
  * group 6: High (&gt;60%) reviews and sport only — "Traditional Media Adjacent + Sport"?




---

class: inverse, middle

### With Cluster Analysis you can use the derived clusters for anything: e.g. interesting visualisations or further regression models on other variables.

---

class: inverse, middle

Week 10: Cluster Analysis — Summary &amp; Practical
# Summary &amp; Practical.


---

class: middle

## Summary

* Cluster Analysis can be used in quantitative social research when we are __interested in identifying an underlying grouping — or typology — of observations__.

--

* There are multiple forms of cluster analysis that can be used for defining clusters under different conditions: differently shaped clusters, different types of hierarchy in their definition, and different types of data. We have learned how two of these __k-means__ and __Hierarchical Cluster Analysis (HCA)__ can be estimated using `R`.

--

* Generally speaking, in Cluster analysis, we need to:
  * __Choose the most appropriate method__ (k-means or HCA) and measurements (distance metric &amp; clustering method for HCA)
  * Use data visualisation and Silhouette/Gap statistics (in k-means) to __decide on a number of underlying clusters__
  * __Describe our clusters__ using bivariate statistics (we only used summary tables here, but we could also use data visualisations and other tools to show variation, ranges, different strength of correlations, etc.)


---

class: middle

## Practical

#### Do US states and English Community Safety Partnerships fall into clearly defined clusters of types and scales of criminal offences committed?

* Download and extract the data and .Rmd files from the `week-10-exercise.zip`. Open the exercise Rmarkdown file and follow the instructions to complete the practical.

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"sealed": false,
"ratio": "16:9",
"self_contained": true,
"countIncrementalSlides": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<style>
.my-header {
	background-color: #440099;
	position: fixed;
	top: 0px;
	left: 0px;
	height: 70px;
	width: 100%;
	text-align: left;
}
</style>

<script>
document
  .querySelectorAll(
    '.remark-slide-content' +
    ':not(.inverse)' +
    // add additional classes to exclude here, e.g.
    // ':not(.inverse)' +
    ':not(.hide-logo)'
  )
  .forEach(el => {
    el.innerHTML += '<div class="my-header"></div>';
  });
</script>


<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
