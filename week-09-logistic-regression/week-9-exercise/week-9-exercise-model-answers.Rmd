---
title: "Week 9 Exercise - Multiple Logistic Regression Model Answers"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

This week we are learning how to estimate logistic regression models in R. For this example, we will be using data from the US Census and the Southern Poverty Law Center about active hate groups in the United States (source: https://www.splcenter.org/hate-map). 

In this practical we are going to explore which state-level variables are associated with higher or lower likelihoods of the state having at least one active Neo-Nazi or anti-immigration hate group in 2020. 


```{r}

# Load packages we will be using: remember to install them
# first with install.packages() if you do not have them.
library(tidyverse) # for data tidying and plotting
library(ggeffects) # for partial plots
library(caret) # for testing accuracy
library(performance) # for testing accuracy through k-fold validation
library(broom) # for turning model summaries into tibbles with tidy()

# Read in the data sample
hate_data <- read_rds("hate_group_data.rds")

# Print a preview of the data
hate_data

```

---

## Part I

## Logistic Regression: Predicting States with Active Neo-Nazi Hate Groups

In this case, the researchers want to see how the percentage of the population with Bachelors or higher levels of education (`pc_bachelors_ed`), the percentage of the population aged 65 plus (`pc_age_65_plus`), the percentage of the population who identify as black in the US census (`pc_black_pop`), the latitude of the state (to see if hate groups are more common in Southern states or Northern States) (`latitude`) and the percentage of households in the state with access to the internet (`pc_hh_with_internet`), change the probabilities of a state having an active hate group. They also want to control for the size of the population (in 100,000s) (`population`).

They create a logistic regression model using the `glm()` function and a `family = binomial(link = "logit")` argument.

```{r}

model_1 <- glm(data = hate_data,
                  formula = neo_nazi_hgs ~ 
                              pc_bachelors_ed + population + pc_age_65_plus +
                              pc_black_pop + pc_hh_with_internet + latitude,
                  family = binomial(link = "logit"))

summary(model_1)

```

Remember that, in this case we have data for all US states in 2020 - this means we may not be particularly interested in the p-values and are more interested in the effects of each variable (as we may not be trying to generalise to a larger population).

* Using the `exp()` function to create exponentiated odds, describe how changes in the percentage of the population with bachelors or higher education (`pc_bachelors_ed`) is associated with changes in the likelihood of a state having an active Neo-Nazi hate group.

Each additional percentage point of the population with bachelors or higher level education was associated with a 16% reduction in the likelihood of the state having an active Neo-Nazi hate group. 


* Using the `exp()` function to create exponentiated odds, describe how changes in the population size (`population`) is associated with changes in the likelihood of a state having an active Neo-Nazi hate group.

Every 100,000 people in the population of a state was associated with a 3.34% increase in the odds of the state having an active Neo-Nazi hate group. 



* Using the `exp()` function to create exponentiated odds, describe how changes in the percentage of people aged 65 or over (`pc_age_65_plus`) is associated with changes in the likelihood of a state having an active Neo-Nazi hate group.

Each 1 percentage point increase in the percentage of the population aged 65 or over was associated with a 11.22% increase in the odds of the state having an active Neo-Nazi hate group.



* Using the `exp()` function to create exponentiated odds, describe how changes in the percentage of the population who identify as black in the census (`pc_black_pop`) is associated with changes in the likelihood of a state having an active Neo-Nazi hate group.

Each 1 percentage point increase in the percentage of the population identifying as black in the US census was associated with a 6.9% decrease in the odds of the state having an active Neo-Nazi hate group.



* Using the `exp()` function to create exponentiated odds, describe how changes in the percentage of households with access to the internet (`pc_hh_with_internet`) is associated with changes in the likelihood of a state having an active Neo-Nazi hate group.

Each 1 percentage point increase in the percentage of households with access to the internet was associated with a 50.7% increase in the odds of the state having an active Neo-Nazi hate group.



* Lastly, using the `exp()` function to create exponentiated odds, describe how changes in the latitude of the state's capital (`latitude`) is associated with changes in the likelihood of a state having an active Neo-Nazi hate group. Keep in mind that an increase of 1 degree in latitude means the state is further North. 

Each 1 degree increase in the latitude (further North) of a state's capital was associated with a 3.37% reduction in the odds of the state having an active Neo-Nazi hate group.


---

They decide to tidy their model using `broom::tidy()` so that they can create a new column with the exponentiated odds for each independent variable using the `mutate` function.

```{r}

# Tidy model
model_1_tidy <- broom::tidy(model_1)

# Add exponentiated odds
model_1_tidy %>%
  mutate(
    exp_odds = exp(estimate)
  )

```

Note that these figures are written in scientific notation. 8.37e- 1 is equal to 0.837; 1.03e+ 0 is equal to 1.03; 9.31e- 1 is equal to 0.931; and so on.

* Check that your answers above match the exponentiated odds generated above. If they differ, why do they differ? This will also help you practice interpreting scientific notation.


---

Next, the researchers decide to try and plot the predicted probabilities for a few of their variables: `pc_bachelors_ed`, `population`, `pc_hh_with_internet`, and `latitude`. They use the `ggeffects` package to achieve this.

```{r}

# ggeffects:ggeffect can quickly plot predicted probabilities for both categorical
# and continuous predictors - especially useful for logistic regression
# models! Note that any plot you create with ggeffect can be added to using 
# standard ggplot functions, like ggtitle, xlab, or theme_classic!
ggeffect(model_1, terms = "pc_bachelors_ed") %>%
  plot() +
  ggtitle("Predicted probability of an active Neo Nazi Hate Group") +
  xlab("Percentage with Bachelors or Higher Education") +
  theme_minimal()

```


* What is the predicted probability (approximately) of active Neo-Nazi hate groups in states with a bachelors degree/higher education rate of 25%?

Around 62.5%.


* What is the predicted probability (approximately) of a state having active Neo-Nazi hate groups in states with a bachelors degree or higher education rate of 40%?

Around 12.5%.


---

```{r}

ggeffect(model_1, terms = "population") %>%
  plot() 

```

* Add a sensible title and X and Y labels to the plot to make it look nicer, using added ggplot code.

```{r}

ggeffect(model_1, terms = "population") %>%
  plot() +
  ggtitle("Predicted probability of an active Neo Nazi Hate Group") +
  xlab("Population (100,000s)") +
  ylab("Predicted Probability of Active Neo-Nazi Hate Group")

```


---

* Create a predicted probabilities plot for the remaining two variables of interest: the percentage of households with internet access (`pc_hh_with_internet`) and the latitude of the state capital (`latitude`).

* The effect of the percentage of households with internet access

```{r}

ggeffect(model_1, terms = "pc_hh_with_internet") %>%
  plot() +
  ggtitle("Predicted probability of an active Neo Nazi Hate Group") +
  xlab("Percent of Household with Internet") +
  ylab("Predicted Probability of Active Neo-Nazi Hate Group")

```

* The effect of the latitude of the state capital

```{r}

ggeffect(model_1, terms = "latitude") %>%
  plot() +
  ggtitle("Predicted probability of an active Neo Nazi Hate Group") +
  xlab("Latitude of State Capital") +
  ylab("Predicted Probability of Active Neo-Nazi Hate Group")

```


---

Next, the researchers decide they want to find out how accurate their model is. In other words, if they had information about all of their independent variables, but not about whether there were any active Neo-Nazi hate groups in the state, how accurately would they be able to guess whether there were any? This is one way of assessing model fit when we cannot use something like the R-squared value.

First, they want to use a conventional Confusion Matrix and measurement of accuracy. In order to do this, they first need to calculate the predictions from their model.

---

The `predict` function can be used to generate the predicted probability for all
cases in the sample, based on the model. This is added as a new column to the data
using the mutate function

```{r}

hate_data <- hate_data %>%
  mutate(
    predictions = predict(model_1, type = "response", newdata = hate_data)
  )


```

You can plot a simple histogram of the predictions to see how they look. They should look like a spread of proportions (between 0 and 1).

```{r}

hist(hate_data$predictions)

```

We want to test how many predictions would have been correct - to do this we are going to say that predictions equal to 50% or greater will be our model predicting that the state *does* have an active Neo-Nazi group (1), whereas predictions less than 50% will be our model predicting that the state *does not* have an active Neo-Nazi group (0)

```{r}

hate_data <- hate_data %>%
  mutate(
    # Overwrite the predictions variable where prediction >= 0.5 is 1 and
    # a prediction less than 0.5 is 0. Keep missing as missing
    predictions = case_when(is.na(predictions) ~ NA_real_, 
                           predictions >= 0.5 ~ 1,
                           TRUE ~ 0
                           )
  ) 


```

We can use the 'confusionMatrix()' function from the `caret` package to calculate the accuracy of our model predictions.

```{r}

caret::confusionMatrix(data = factor(hate_data$predictions),
                       reference = factor(hate_data$neo_nazi_hgs))

```

The caret confusion matrix gives us quite a lot of information, but the important parts for our purposes are: the two-by-two table at the top, the Accuracy value, the No Information Rate, and the probability that the model's accuracy is better than the No Information Rate (P-Value [Acc > NIR]).

__The Confusion Matrix__
The 2x2 table tells us how our predictions (rows) matched up with or deviated from our real values. In this case, our model was able to correctly predict 12 of the 19 states with active Neo-Nazi hate groups. It was also able to correctly identify 28 of the 32 states that did not have active Neo-Nazi hate groups.

__Accuracy__
The accuracy value gives us the proportion of cases in our data that were correctly classified by the logistic regression model. Here, our model correctly classified around 78.4% of states as having or not having active Neo-Nazi hate groups.

__No Information Rate__
The No Information Rate tells us the proportion of the largest group (0 or 1) we would expect to be able to get correct just by knowing the proportion of states that had or did not have Neo-Nazi hate groups, on average, and then randomly guessing for each state. In this case, if we had just randomly guessed we would be expected to have correctly classified around 62.75% of the largest group (states without active Neo-Nazi hate groups). 

__P-Value [Acc > NIR]__
This is the p-value for whether our model performs significantly better than what we would expect from the random range of correct classifications we would expect to get with no information. If it is less than 0.05, we could reject the idea that our model is no better than just randomly guessing.

---

A confusion matrix is not the only way we can assess accuracy. We can also use something called k-fold cross-validation. K-fold cross validation provides a measure of accuracy by randomly splitting the sample into a subset of mutually exclusive groups (k subsets) before checking how well the model performs on all of those different groups on average.

Because we have such a small sample size it wouldn't be wise for us to choose a high number of 'folds', but we can still check accuracy with the k-fold cross-validation algorithm using the `performance` R package.

```{r}

# Here I set the random number generator using set.seed() so that the whole class gets the same results
# because the k-folds will be chosen in the same way
set.seed(10000)

# Then I run the performance_accuracy function from the performance package
performance::performance_accuracy(model_1, k = 3)

```

The k-fold cross-validation seems to be telling us approximately the same thing as our conventional Confusion Matrix accuracy test - the average accuracy across folds was around 78.73%. However, we probably can't be too sure in this case because our population is so small — generally a good number of k-folds is 10, but if we tried to do this with our data it would mean only around 5 cases would be in each fold. 

---

Finally, the researchers checked for any issues of multicollinearity in their independent variables. The `performance` package includes a function to check this in much the same way as the `car` package.

```{r}

performance::multicollinearity(model_1)

```

* Was there any evidence of problematic multicollinearity in this model, based on the VIF statistics?

The VIF statistics were all below 5, which is generally considered a cut off for problematic multicollinearity. 


We can also check for outliers using the same plot function we used in multiple linear regression. The `performance` package also has a function to check for outliers, but it is not always very informative.

```{r}

plot(model_1, which = 4)
plot(model_1, which = 5)

performance::check_outliers(model_1)

```

* Was there any evidence of outliers or leverage points? If so, which states may be outliers and what would you recommend doing to ensure the model estimates are not bias?

The plots seem to suggest that state 12 (Hawaii), state 34 (Minnesota), and state 46 (Utah) may be substantial leverage points or outliers, however, the performance method of checking for outliers does not suggest any problematic outliers. It may be beneficial to try re-running the model with the outliers removed in order to check that they are not disproportionately impacting the results. 


Lastly, the researchers check whether there is any evidence of autocorrelation using a Durbin-Watson test.

```{r}

car::durbinWatsonTest(model_1)

```

* Was there any evidence of significant autocorrelation within the residuals from the model?

There was no significant evidence of autocorrelation among the residuals in the model. The Durbin Watson test statistic had an associated p-value that was greater than 0.05, which means we cannot reject the null hypothesis that the residuals are independent. 



---

## Part II

## Logistic Regression: Predicting States with Active Anti-Immigrant Hate Groups

* Using the above code as a template, create a similar logistic regression model predicting whether states have an active anti-immigrant hate group below. 

```{r}

model_2 <- glm(data = hate_data,
                  formula = anti_imig_hgs ~ 
                              pc_bachelors_ed + population + pc_age_65_plus +
                              pc_black_pop + pc_hh_with_internet + latitude,
                  family = binomial(link = "logit"))

summary(model_2)

```

* Use the `broom::tidy()` function to create a tidy tibble version of your model results, and then create a new column in this tibble for the exponentiated (`exp()`) coefficients of the changes in log odds.

```{r}

model_2_tidy <- broom::tidy(model_2)

model_2_tidy <- model_2_tidy %>%
  mutate(
    exp_odds = exp(estimate)
  )

model_2_tidy

```

* Interpret the coefficients in your model: how are changes in each independent variable associated with changes in the log odds of a state having an active anti-immigrant hate group?

An increase of 1 percentage point in the percentage of the population with Bachelors or higher level education was associated with a 1.3 times increase in the odds of a state having an active anti-immigrant hate group. An increase in the population size of a state of 100,000 was associated with a further increase of around 3%. An increase of 1 percentage point in the percentage of the population who are aged 65+ was associated with a 7% increase in the odds of an active anti-immigration hate group and increases of 1 percentage point in the percentage of the population identifying as black in the population was associated with a 2% increase in the likelihood of a state having an active anti-immigration hate group. Higher percentages of households with access to the internet were associated with only slighly reduced odds of a state having an active anti-immigration hate group (around a 0.6% decrease per 1 percentage point increase in the percentage of households with access to the internet). Lastly, an increase of 1 degree in state capital latitude (indicating a more Northern state) was associated with a 26.5 per cent decrease in the likelihood of a state having an active anti-immigrant hate group. 


---

* Pick two of the results that you find particularly interesting and create partial plots showing the predicted probabilities of a state having an active anti-immigrant hate group across values of these variables.

```{r}

ggeffect(model_2, terms = "latitude") %>%
  plot() 

```

```{r}

ggeffect(model_2, terms = "pc_bachelors_ed") %>%
  plot() 

```

---

Mutate the predictions required to run the `confusionMatrix()` function from the `caret` package, and then run this function. 

```{r}

hate_data <- hate_data %>%
  mutate(
    predictions_2 = predict(model_2, type = "response", newdata = hate_data)
  )

hist(hate_data$predictions_2)

hate_data <- hate_data %>%
  mutate(
    # Overwrite the predictions variable where prediction >= 0.5 is 1 and
    # a prediction less than 0.5 is 0. Keep missing as missing
    predictions_2 = case_when(is.na(predictions) ~ NA_real_, 
                           predictions >= 0.5 ~ 1,
                           TRUE ~ 0
                           )
  ) 

caret::confusionMatrix(data = factor(hate_data$predictions_2),
                       reference = factor(hate_data$anti_imig_hgs))


```

* Report the results of the confusion matrix, the accuracy rate, the no information rate, and the p-value associated with the difference between your model's accuracy and the no information accuracy rate.

The confusion matrix for this model shows that the logistic regression produced a large number of false-positives, it predicted that 12 of the 46 states without active anti-immigrant hate groups actually did have active anti-immigration hate groups. However, it did successfully identify 4 of the 5 states that did have active anti-immigrant hate groups. Overall, this resulted in an accuracy of 74.51%, which was significantly worse than the no information rate (90.2%, p = 0.9997). None of the independent variables in the model therefore appear to be very effective at consistently identifying which states have active anti-immigration hate groups. 


---

Try assessing your model's accuracy using k-fold validation with the `performance::performance_accuracy()` function from the `performance` package.

```{r}

set.seed(10000)
performance::performance_accuracy(model_2, k = 3)

```


---

We also need to check our assumptions. First, check whether there are any issues with multicollinearity in your model.

```{r}

performance::multicollinearity(model_2)

```

* Report the VIF results below. Are there any problems with multicollinearity in the model? If so, why? If not, why not?

While none of the independent variables had a VIF value greater than 5, two variables had VIF values that were approaching 5 (pc_bachelors_ed & pc_black_pop). It may be worthwhile removing one of these variables and re-running the model to see if estimates change substantially or if model fit improves.


Next, write the code to check whether there are any outliers using both the `performance` package and the base `plot()` package.

```{r}

plot(model_2, which = 4)
plot(model_2, which = 5)

performance::check_outliers(model_2)

```

* Are there any states you might consider outliers? If there are potential outliers, what would you recommend doing to check if these are unduly influencing the estimates of some independent variables?

The Cook's Distance plots seem to suggest that observation 9 (District of Columbia (DC)), observation 10 (Florida), and observation 12 (Hawaii) may be significant outliers that are biasing model estimates. The output from the performance package also identifies DC and Hawaii as significant outliers. It may be beneficial to attempt to re-estimate the model with these outliers removed to observe how estimates change. 


Lastly, check whether the residuals from this model are independent (that there is no autocorrelation). You can do this using either the `car` or the `performance` packages.

```{r}

performance::check_autocorrelation(model_2)
car::durbinWatsonTest(model_2)

```

The Durbin Watson test statistic indicates that there is no significant evidence of autocorrelation (D-W Statistic = 1.78, p = 0.41).



---

## Week 9 Challenge

* For more practice, create a new binary categorical variable where 1 is equal to a state having neither an active Neo Nazi hate group, nor an active anti-immigration hate group, and 0 is equal to the state having either an active Neo-Nazi hate group, or an active anti-immigration hate group, or both. Then, create a logistic regression model predicting states that have neither active anti-immigration nor active Neo-Nazi hate groups and report the results. 

However, if you feel like you'd prefer to challenge yourself by looking at some data you might use for assessment two feel free to do that instead!



